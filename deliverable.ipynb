{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e78906",
   "metadata": {},
   "source": [
    "This is a self-contained notebook to re-produce the experiments. The original code has been modified in some cases to be easier to re-produce and log. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4a89c",
   "metadata": {},
   "source": [
    "As a first step, create a .env at the root folder and specify the root path. This can be done through\n",
    "```mv .env.defaults .env```\n",
    " and specify ```\n",
    "PROJECT_PATH```\n",
    " in ```\n",
    ".env```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba72902",
   "metadata": {},
   "source": [
    "The first script generates a dataset of 110,000 synthetic molecular structures (SMILES strings) using a pretrained Variational Autoencoder (VAE). \n",
    "\n",
    "It computes chemical properties like penalized logP, QED, and synthetic accessibility, or binding affinities for specific proteins, processing data in batches and using parallel computation. The generated molecular representations and properties are saved to a PyTorch file and a CSV file.\n",
    "\n",
    "The current setup saves the two generated files at data/interim/props\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86923ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991512a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ChemFlow/data/processed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ChemFlow.experiments.prepare_random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ad720",
   "metadata": {},
   "source": [
    "The next script will train the predictors for each prop. You can set your wandb for logging in experiments/supervised/train_prop_predictor.sh\n",
    "\n",
    "It will have extensive logging, but I believe this is needed to see if everything was executed correctly, so I kept it. \n",
    "\n",
    "The checkpoints for the different prop predictors will be stored at ChemFlow/checkpoints/prop_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ChemFlow/experiments/supervised/train_prop_predictor.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1630dbc",
   "metadata": {},
   "source": [
    "The following cell will Train the energy network with supervised semantic guidance\n",
    "\n",
    "The output will be located at checkpoints/wavepde_prop\n",
    "\n",
    "Make sure to change the wandb entity in the .sh below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ChemFlow/experiments/supervised/train_wavepde_prop.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa773e2e",
   "metadata": {},
   "source": [
    "Train the energy network with unsupervised diversity guidance\n",
    "\n",
    "The output model will be stored at checkpoints/wavepde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ChemFlow.experiments.train_wavepde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b155801",
   "metadata": {},
   "source": [
    "Compute the pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ChemFlow.experiments.unsupervised.corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f324c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading data from: ChemFlow/data/interim/corr/wave_unsup_0.1.csv\n",
      "✅ Loaded DataFrame shape: (100000, 12)\n",
      "📊 Analyzing properties: ['plogp', 'sa', 'qed', 'drd2', 'jnk3', 'gsk3b', 'uplogp']\n",
      "🔄 Calculating correlations for each trajectory (k_idx, idx)...\n",
      "   (Using pandarallel)\n",
      "🔄 Calculating mean correlations per k_idx...\n",
      "\n",
      "📋 Mean Correlations per k_idx:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plogp</th>\n",
       "      <th>sa</th>\n",
       "      <th>qed</th>\n",
       "      <th>drd2</th>\n",
       "      <th>jnk3</th>\n",
       "      <th>gsk3b</th>\n",
       "      <th>uplogp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k_idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.002975</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.003674</td>\n",
       "      <td>0.024355</td>\n",
       "      <td>0.023142</td>\n",
       "      <td>-0.010233</td>\n",
       "      <td>0.029134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008136</td>\n",
       "      <td>-0.032025</td>\n",
       "      <td>-0.008560</td>\n",
       "      <td>0.025166</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>-0.028918</td>\n",
       "      <td>0.067936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016134</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>0.012332</td>\n",
       "      <td>0.028598</td>\n",
       "      <td>0.018656</td>\n",
       "      <td>-0.010438</td>\n",
       "      <td>0.032703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023399</td>\n",
       "      <td>0.022573</td>\n",
       "      <td>-0.010237</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>-0.002295</td>\n",
       "      <td>0.014903</td>\n",
       "      <td>0.004804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002360</td>\n",
       "      <td>0.015832</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>-0.005333</td>\n",
       "      <td>-0.016061</td>\n",
       "      <td>-0.025026</td>\n",
       "      <td>0.009768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.022470</td>\n",
       "      <td>0.016017</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>-0.026228</td>\n",
       "      <td>0.031234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010143</td>\n",
       "      <td>-0.028732</td>\n",
       "      <td>0.067395</td>\n",
       "      <td>-0.002777</td>\n",
       "      <td>-0.019771</td>\n",
       "      <td>-0.023012</td>\n",
       "      <td>0.064095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.016611</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>-0.019142</td>\n",
       "      <td>0.038507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011089</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.009207</td>\n",
       "      <td>0.022914</td>\n",
       "      <td>-0.002373</td>\n",
       "      <td>-0.029472</td>\n",
       "      <td>0.024719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.049155</td>\n",
       "      <td>-0.038415</td>\n",
       "      <td>0.014769</td>\n",
       "      <td>0.019796</td>\n",
       "      <td>-0.007791</td>\n",
       "      <td>-0.017929</td>\n",
       "      <td>0.062484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          plogp        sa       qed      drd2      jnk3     gsk3b    uplogp\n",
       "k_idx                                                                      \n",
       "0     -0.002975 -0.004725 -0.003674  0.024355  0.023142 -0.010233  0.029134\n",
       "1      0.008136 -0.032025 -0.008560  0.025166  0.006753 -0.028918  0.067936\n",
       "2      0.016134  0.008053  0.012332  0.028598  0.018656 -0.010438  0.032703\n",
       "3      0.023399  0.022573 -0.010237  0.023322 -0.002295  0.014903  0.004804\n",
       "4      0.002360  0.015832  0.002642 -0.005333 -0.016061 -0.025026  0.009768\n",
       "5      0.001941  0.022470  0.016017 -0.005298  0.007224 -0.026228  0.031234\n",
       "6      0.010143 -0.028732  0.067395 -0.002777 -0.019771 -0.023012  0.064095\n",
       "7      0.022018  0.003450  0.001336  0.016611  0.021571 -0.019142  0.038507\n",
       "8      0.011089 -0.000748 -0.009207  0.022914 -0.002373 -0.029472  0.024719\n",
       "9      0.049155 -0.038415  0.014769  0.019796 -0.007791 -0.017929  0.062484"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Calculating best k_idx for each property (based on max absolute correlation)...\n",
      "\n",
      "🏆 Best k_idx found:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best k_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>plogp</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sa</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qed</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drd2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jnk3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsk3b</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uplogp</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Best k_idx\n",
       "plogp            9\n",
       "sa               9\n",
       "qed              6\n",
       "drd2             2\n",
       "jnk3             0\n",
       "gsk3b            8\n",
       "uplogp           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Best `k_idx` mapping dictionary:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Copy and paste this into the appropriate MAP in `experiments/utils/traversal_step.py`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"drd2\": 2,\n",
      "    \"gsk3b\": 8,\n",
      "    \"jnk3\": 0,\n",
      "    \"plogp\": 9,\n",
      "    \"qed\": 6,\n",
      "    \"sa\": 9,\n",
      "    \"uplogp\": 1,\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_file_path = \"ChemFlow/data/interim/corr/wave_unsup_0.1.csv\" \n",
    "\n",
    "use_binding_affinity = False \n",
    "# --- End Configuration ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Suppress specific warnings if needed (e.g., from pearsonr with constant input)\n",
    "warnings.filterwarnings(\"ignore\", message=\"An input array is constant; the correlation coefficient is not defined.\")\n",
    "\n",
    "# Define the property sets\n",
    "PROPS_STANDARD = [\"plogp\", \"sa\", \"qed\", \"drd2\", \"jnk3\", \"gsk3b\", \"uplogp\"]\n",
    "PROPS_BINDING = [\"1err\", \"2iik\"]\n",
    "\n",
    "def calculate_correlation_for_group(group: pd.DataFrame, props: list) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the Pearson correlation between time step 't' and\n",
    "    each property for a given trajectory group.\n",
    "\n",
    "    Args:\n",
    "        group: DataFrame subset for a specific (k_idx, idx) pair.\n",
    "        props: List of property column names to calculate correlation for.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series containing the correlation coefficient for each property.\n",
    "        Returns NaN for a property if correlation cannot be calculated.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    # Ensure 't' is sorted if not already, though groupby usually preserves order from source\n",
    "    group = group.sort_values('t')\n",
    "    time_steps = group['t'].values\n",
    "\n",
    "    if len(time_steps) < 2: # Need at least 2 points for correlation\n",
    "        for prop in props:\n",
    "            results[prop] = np.nan\n",
    "        return pd.Series(results)\n",
    "\n",
    "    for prop in props:\n",
    "        prop_values = group[prop].values\n",
    "        # Check for constant values or insufficient data points\n",
    "        if len(prop_values) < 2 or np.std(prop_values) < 1e-9: # Check standard deviation\n",
    "             results[prop] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                # Calculate correlation between time steps (e.g., 0..9) and property values\n",
    "                correlation, _ = pearsonr(time_steps, prop_values)\n",
    "                results[prop] = correlation if not np.isnan(correlation) else np.nan\n",
    "            except ValueError: # Should be caught by std check, but just in case\n",
    "                results[prop] = np.nan\n",
    "\n",
    "    return pd.Series(results)\n",
    "\n",
    "def find_best_indices(csv_path: str, binding_affinity: bool) -> (dict, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Loads the correlation data CSV, calculates correlations, finds the best\n",
    "    k_idx for each property, and returns the result as a dictionary\n",
    "    along with the mean correlations DataFrame.\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path to the CSV file generated by corr.py.\n",
    "        binding_affinity: True if binding affinity properties were calculated,\n",
    "                          False otherwise.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "          - A dictionary mapping property names to the best k_idx.\n",
    "          - A DataFrame with the mean correlations per k_idx.\n",
    "        Returns (None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "    csv_file = Path(csv_path)\n",
    "    if not csv_file.is_file():\n",
    "        print(f\"❌ Error: CSV file not found at {csv_path}\", file=sys.stderr)\n",
    "        return None, None\n",
    "\n",
    "    print(f\"🔄 Loading data from: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, index_col=0)\n",
    "        print(f\"✅ Loaded DataFrame shape: {df.shape}\")\n",
    "        # display(df.head()) # Optional: uncomment to see the head\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading CSV file: {e}\", file=sys.stderr)\n",
    "        return None, None\n",
    "\n",
    "    # Determine which properties to analyze\n",
    "    props_to_analyze = PROPS_BINDING if binding_affinity else PROPS_STANDARD\n",
    "    print(f\"📊 Analyzing properties: {props_to_analyze}\")\n",
    "\n",
    "    # Verify required columns exist\n",
    "    required_cols = ['k_idx', 'idx', 't'] + props_to_analyze\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"❌ Error: Missing required columns in CSV: {missing_cols}\", file=sys.stderr)\n",
    "        print(f\"❓ Did you run corr.py with the correct '--binding_affinity' setting ({binding_affinity})?\", file=sys.stderr)\n",
    "        return None, None\n",
    "    print(\"🔄 Calculating correlations for each trajectory (k_idx, idx)...\")\n",
    "    try:\n",
    "        from pandarallel import pandarallel\n",
    "        pandarallel.initialize(nb_workers=os.cpu_count(), progress_bar=False, verbose=0) # Safe to call multiple times\n",
    "        apply_method = df.groupby(['k_idx', 'idx']).parallel_apply\n",
    "        print(\"   (Using pandarallel)\")\n",
    "    except ImportError:\n",
    "        print(\"   (Pandarallel not found, using standard apply - may be slow)\")\n",
    "        apply_method = df.groupby(['k_idx', 'idx']).apply\n",
    "\n",
    "    correlations = apply_method(\n",
    "        calculate_correlation_for_group, props=props_to_analyze\n",
    "    )\n",
    "\n",
    "    # Filter out potential all-NaN groups if apply returned them\n",
    "    correlations.dropna(how='all', inplace=True)\n",
    "\n",
    "    if correlations.empty:\n",
    "        print(\"❌ Error: No valid correlations could be calculated. Check input data and properties.\", file=sys.stderr)\n",
    "        return None, None\n",
    "\n",
    "    print(\"🔄 Calculating mean correlations per k_idx...\")\n",
    "    # Group by k_idx and find the mean correlation across all trajectories for that k_idx\n",
    "    mean_correlations = correlations.groupby('k_idx')[props_to_analyze].mean() # Select only props columns before mean\n",
    "\n",
    "    # Fill any remaining NaNs (e.g., if a k_idx had only invalid trajectories) with 0\n",
    "    mean_correlations.fillna(0, inplace=True)\n",
    "\n",
    "    print(\"\\n📋 Mean Correlations per k_idx:\")\n",
    "    display(mean_correlations)\n",
    "\n",
    "    print(\"\\n🔄 Calculating best k_idx for each property (based on max absolute correlation)...\")\n",
    "    # Calculate absolute correlations\n",
    "    abs_mean_correlations = mean_correlations.abs()\n",
    "\n",
    "    # Find the k_idx (index) corresponding to the maximum absolute correlation for each property (column)\n",
    "    best_indices = abs_mean_correlations.idxmax()\n",
    "\n",
    "    print(\"\\n🏆 Best k_idx found:\")\n",
    "    display(best_indices.to_frame(name='Best k_idx'))\n",
    "\n",
    "    return best_indices.to_dict(), mean_correlations\n",
    "\n",
    "def format_dict_for_code(result_dict: dict) -> str:\n",
    "    \"\"\"Formats the dictionary nicely for pasting into Python code.\"\"\"\n",
    "    if not result_dict:\n",
    "        return \"{}\"\n",
    "    lines = [\"{\"]\n",
    "    # Sort items for consistent output (optional)\n",
    "    sorted_items = sorted(result_dict.items())\n",
    "    for key, value in sorted_items:\n",
    "        lines.append(f'    \"{key}\": {value},')\n",
    "    lines.append(\"}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# --- Run the analysis ---\n",
    "best_k_indices_dict, mean_correlations_df = find_best_indices(csv_file_path, use_binding_affinity)\n",
    "\n",
    "# --- Display the result ---\n",
    "if best_k_indices_dict:\n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(\"**Best `k_idx` mapping dictionary:**\"))\n",
    "    display(Markdown(\"Copy and paste this into the appropriate MAP in `experiments/utils/traversal_step.py`\"))\n",
    "    display(Markdown(\"---\"))\n",
    "    # Print the dictionary in a format easy to copy\n",
    "    print(format_dict_for_code(best_k_indices_dict))\n",
    "    display(Markdown(\"---\"))\n",
    "else:\n",
    "    print(\"❌ Analysis failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244e8b2d",
   "metadata": {},
   "source": [
    "With this above we got the indices for wave pde. We want to do the same for hj_unsup. To do that, we need to edit in-place the corr.py line 31 and re-run. Also, first traine the hj model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ChemFlow.experiments.train_wavepde --model.pde_function hj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b50110",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ChemFlow.experiments.unsupervised.corr --method hj_unsup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1107ec1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading data from: ChemFlow/data/interim/corr/hj_unsup_0.1.csv\n",
      "✅ Loaded DataFrame shape: (100000, 12)\n",
      "📊 Analyzing properties: ['plogp', 'sa', 'qed', 'drd2', 'jnk3', 'gsk3b', 'uplogp']\n",
      "🔄 Calculating correlations for each trajectory (k_idx, idx)...\n",
      "   (Using pandarallel)\n",
      "🔄 Calculating mean correlations per k_idx...\n",
      "\n",
      "📋 Mean Correlations per k_idx:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plogp</th>\n",
       "      <th>sa</th>\n",
       "      <th>qed</th>\n",
       "      <th>drd2</th>\n",
       "      <th>jnk3</th>\n",
       "      <th>gsk3b</th>\n",
       "      <th>uplogp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k_idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100063</td>\n",
       "      <td>-0.264589</td>\n",
       "      <td>0.312847</td>\n",
       "      <td>-0.207715</td>\n",
       "      <td>-0.103233</td>\n",
       "      <td>-0.165283</td>\n",
       "      <td>0.380999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.109219</td>\n",
       "      <td>-0.245072</td>\n",
       "      <td>0.173280</td>\n",
       "      <td>-0.212454</td>\n",
       "      <td>-0.097415</td>\n",
       "      <td>-0.165358</td>\n",
       "      <td>0.312019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.115744</td>\n",
       "      <td>-0.379317</td>\n",
       "      <td>0.315582</td>\n",
       "      <td>-0.166805</td>\n",
       "      <td>-0.148770</td>\n",
       "      <td>-0.219049</td>\n",
       "      <td>0.486597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004018</td>\n",
       "      <td>-0.031678</td>\n",
       "      <td>0.007175</td>\n",
       "      <td>0.021101</td>\n",
       "      <td>-0.015395</td>\n",
       "      <td>-0.029828</td>\n",
       "      <td>-0.002539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.021478</td>\n",
       "      <td>-0.008729</td>\n",
       "      <td>0.207988</td>\n",
       "      <td>-0.072907</td>\n",
       "      <td>-0.079255</td>\n",
       "      <td>-0.015099</td>\n",
       "      <td>0.102165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.032483</td>\n",
       "      <td>-0.238833</td>\n",
       "      <td>0.363127</td>\n",
       "      <td>-0.093270</td>\n",
       "      <td>-0.080534</td>\n",
       "      <td>-0.124353</td>\n",
       "      <td>0.420318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.016905</td>\n",
       "      <td>-0.027426</td>\n",
       "      <td>0.052943</td>\n",
       "      <td>-0.029358</td>\n",
       "      <td>-0.054390</td>\n",
       "      <td>0.044048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.064343</td>\n",
       "      <td>-0.257312</td>\n",
       "      <td>0.419164</td>\n",
       "      <td>-0.170422</td>\n",
       "      <td>-0.102255</td>\n",
       "      <td>-0.171788</td>\n",
       "      <td>0.484602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.010319</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.148722</td>\n",
       "      <td>-0.084167</td>\n",
       "      <td>-0.048704</td>\n",
       "      <td>0.041201</td>\n",
       "      <td>0.073444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.230213</td>\n",
       "      <td>-0.434614</td>\n",
       "      <td>0.387901</td>\n",
       "      <td>-0.382590</td>\n",
       "      <td>-0.216325</td>\n",
       "      <td>-0.328923</td>\n",
       "      <td>0.487114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          plogp        sa       qed      drd2      jnk3     gsk3b    uplogp\n",
       "k_idx                                                                      \n",
       "0      0.100063 -0.264589  0.312847 -0.207715 -0.103233 -0.165283  0.380999\n",
       "1      0.109219 -0.245072  0.173280 -0.212454 -0.097415 -0.165358  0.312019\n",
       "2      0.115744 -0.379317  0.315582 -0.166805 -0.148770 -0.219049  0.486597\n",
       "3      0.004018 -0.031678  0.007175  0.021101 -0.015395 -0.029828 -0.002539\n",
       "4     -0.021478 -0.008729  0.207988 -0.072907 -0.079255 -0.015099  0.102165\n",
       "5      0.032483 -0.238833  0.363127 -0.093270 -0.080534 -0.124353  0.420318\n",
       "6      0.048200  0.016905 -0.027426  0.052943 -0.029358 -0.054390  0.044048\n",
       "7      0.064343 -0.257312  0.419164 -0.170422 -0.102255 -0.171788  0.484602\n",
       "8     -0.010319  0.005989  0.148722 -0.084167 -0.048704  0.041201  0.073444\n",
       "9      0.230213 -0.434614  0.387901 -0.382590 -0.216325 -0.328923  0.487114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Calculating best k_idx for each property (based on max absolute correlation)...\n",
      "\n",
      "🏆 Best k_idx found:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best k_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>plogp</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sa</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qed</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drd2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jnk3</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsk3b</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uplogp</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Best k_idx\n",
       "plogp            9\n",
       "sa               9\n",
       "qed              7\n",
       "drd2             9\n",
       "jnk3             9\n",
       "gsk3b            9\n",
       "uplogp           9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Best `k_idx` mapping dictionary:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Copy and paste this into the appropriate MAP in `experiments/utils/traversal_step.py`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"drd2\": 9,\n",
      "    \"gsk3b\": 9,\n",
      "    \"jnk3\": 9,\n",
      "    \"plogp\": 9,\n",
      "    \"qed\": 7,\n",
      "    \"sa\": 9,\n",
      "    \"uplogp\": 9,\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_file_path = \"ChemFlow/data/interim/corr/hj_unsup_0.1.csv\" \n",
    "\n",
    "use_binding_affinity = False \n",
    "\n",
    "best_k_indices_dict, mean_correlations_df = find_best_indices(csv_file_path, use_binding_affinity)\n",
    "\n",
    "# --- Display the result ---\n",
    "if best_k_indices_dict:\n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(\"**Best `k_idx` mapping dictionary:**\"))\n",
    "    display(Markdown(\"Copy and paste this into the appropriate MAP in `experiments/utils/traversal_step.py`\"))\n",
    "    display(Markdown(\"---\"))\n",
    "    # Print the dictionary in a format easy to copy\n",
    "    print(format_dict_for_code(best_k_indices_dict))\n",
    "    display(Markdown(\"---\"))\n",
    "else:\n",
    "    print(\"❌ Analysis failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fd983",
   "metadata": {},
   "source": [
    "Make sure to replace the indices.\n",
    "\n",
    "The next script will prepare the data for the optimization, the new data will be found under data/interim/props/zinc250k.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ChemFlow.experiments.prepare_data --smiles_file ChemFlow/data/processed/zinc250k.smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852106e4",
   "metadata": {},
   "source": [
    "The next script will run the optimzation. The csvs with the optimized properties will be stored at ChemFlow/data/interim/optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ed089",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ChemFlow/experiments/optimization/optimization.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23a71b",
   "metadata": {},
   "source": [
    "The next script will run unconstrained optimization for plogp and qed.\n",
    "\n",
    "The results will be stored at data/interim/uc_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc511cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ChemFlow/experiments/optimization/uc_optim.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d8fc5",
   "metadata": {},
   "source": [
    "Run the multi-objective optimization.\n",
    "\n",
    "Results will be stored at /data/interim/optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ChemFlow.experiments.optimization.optimization_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d5aa3",
   "metadata": {},
   "source": [
    "Run the molecule manipualtion task.\n",
    "\n",
    "The results will be stored at ChemFlow/experiments/success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff721f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ChemFlow/experiments/success_rate/success_rate.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd934a20",
   "metadata": {},
   "source": [
    "Time for result presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ./ChemFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472fbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "%cd {os.getenv(\"PROJECT_PATH\") or \".\"}\n",
    "import sys\n",
    "sys.path.append('ChemFlow') \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from absl import logging\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from timeit import default_timer as timer\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(\n",
    "    nb_workers=os.cpu_count(),\n",
    "    progress_bar=False,\n",
    "    verbose=0\n",
    ")\n",
    "from rdkit import Chem\n",
    "from ChemFlow.src.utils.scores import *\n",
    "from ChemFlow.src.vae import load_vae\n",
    "from ChemFlow.src.pinn.pde import load_wavepde\n",
    "from ChemFlow.src.pinn import VAEGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_df(df: pd.DataFrame):\n",
    "    display(df.head())\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d134731",
   "metadata": {},
   "source": [
    "Define the files to be used for the optimization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prop = 'plogp'\n",
    "reverse = prop in MINIMIZE_PROPS\n",
    "\n",
    "files = [\n",
    "    (f'ChemFlow/data/interim/optimization/{prop}_random_0.1_absolute.csv', 'Random'),\n",
    "    (f'ChemFlow/data/interim/optimization/{prop}_random_1d_0.1_absolute.csv', 'Random 1D'),\n",
    "    (f'ChemFlow/data/interim/optimization/{prop}_limo_0.1_relative.csv', 'Gradient Flow'),\n",
    "    #(f'ChemFlow/data/interim/optimization/{prop}_chemspace_0.1_absolute.csv', 'ChemSpace'),\n",
    "    (f'ChemFlow/data/interim/optimization/{prop}_wave_sup_0.1_relative.csv',\n",
    "     'Wave eqn. (spv)'),\n",
    "    (f'ChemFlow/data/interim/optimization/{prop}_wave_unsup_0.1_relative.csv',\n",
    "     'Wave eqn. (unsup)'),\n",
    "    (f'ChemFlow/data/interim/optimization/{prop}_hj_sup_0.1_relative.csv', 'HJ eqn. (spv)'),\n",
    "    (f'ChemFlow/data/interim/optimization/{prop}_hj_unsup_0.1_relative.csv',\n",
    "     'HJ eqn. (unsup)'),\n",
    "    (f'ChemFlow/data/interim/optimization/{prop}_fp_0.1_relative.csv', 'Langevin Dynamics'),\n",
    "]\n",
    "\n",
    "\n",
    "results = []\n",
    "for file, name in files:\n",
    "    df_raw = pd.read_csv(file, index_col=0)\n",
    "    df_init = df_raw.query('t == 0')\n",
    "\n",
    "    n = df_init.shape[0]\n",
    "    steps = df_raw.t.max() + 1\n",
    "\n",
    "\n",
    "    def func(x: pd.Series):\n",
    "        mol = Chem.MolFromSmiles(x['smiles'])\n",
    "\n",
    "        if mol is None:\n",
    "            # x['valid'] = False\n",
    "            return x\n",
    "        # x['valid'] = True\n",
    "        if x['t'] == 0:\n",
    "            x['sim'] = 1\n",
    "            x['delta'] = 0\n",
    "        else:\n",
    "            try:\n",
    "                x['sim'] = ssim(x['smiles'], df_init.loc[x['idx'], 'smiles'])\n",
    "                x['delta'] = x[prop] - df_init.loc[x['idx'], prop]\n",
    "            except Exception as e:\n",
    "                # x['valid'] = False\n",
    "                return x\n",
    "        return x\n",
    "\n",
    "\n",
    "    df_imp = df_raw.parallel_apply(func, axis=1).dropna()\n",
    "\n",
    "    file_path = Path(file)\n",
    "    os.makedirs(file_path.parent / 'sims', exist_ok=True)\n",
    "    df_imp.to_csv(file_path.parent / 'sims' / file_path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_files = []\n",
    "for file, name in files:\n",
    "    file_path = Path(file)\n",
    "    sim_files.append((file_path.parent / 'sims' / file_path.name, name))\n",
    "print(sim_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453e746",
   "metadata": {},
   "source": [
    "Calculate the similarity-constrained optimization for the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5af2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 800\n",
    "steps = 1000\n",
    "deltas = torch.zeros((n, steps), device='cuda')\n",
    "results = []\n",
    "\n",
    "for file, name in sim_files:\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "\n",
    "    for sim in [0, 0.2, 0.4, 0.6]:\n",
    "        df = df.query(f'sim >= {sim}')\n",
    "        deltas.zero_()\n",
    "        for _, row in df.iterrows():\n",
    "            deltas[row['idx'], row['t']] = row['delta']\n",
    "        # improvements = torch.cummax(deltas, dim=1).values\n",
    "        # improvements = improvements[:,-1]\n",
    "        improvements = torch.max(deltas, dim=1).values\n",
    "\n",
    "        succ = (improvements > 0).sum().item() / n\n",
    "        improvements = improvements[improvements > 0]\n",
    "        r = f'{improvements.mean().item():.2f} ± {improvements.std().item():.2f} ({succ * 100:.1f})'\n",
    "\n",
    "        print(f'{name:<20} {sim:.1f}: {r}')\n",
    "\n",
    "        results.append({\n",
    "            'name': name,\n",
    "            'sim': sim,\n",
    "            'improvement': r\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "show_df(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd21fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sim in [0, 0.2, 0.4, 0.6]:\n",
    "    row = []\n",
    "    for file, name in sim_files:\n",
    "        row.append(\n",
    "            df_results.query(f'name == \"{name}\" and sim == {sim}').improvement.values[\n",
    "                0])\n",
    "    results.append(row)\n",
    "\n",
    "df_table = pd.DataFrame(results, columns=[name for _, name in sim_files],\n",
    "                        index=[f'{sim:.1f}' for sim in [0, 0.2, 0.4, 0.6]])\n",
    "\n",
    "show_df(df_table)\n",
    "df_table.to_csv(f'ChemFlow/data/interim/optimization/{prop}_improvement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03908e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df\n",
    "r = []\n",
    "\n",
    "for file, name in sim_files:\n",
    "    # with open(file, 'rb') as f:\n",
    "    #     deltas = pickle.load(f)\n",
    "    # for i in range(len(deltas)):\n",
    "    #     for t in range(len(deltas[i][prop])):\n",
    "    #         r.append({\n",
    "    #             'name': name,\n",
    "    #             'idx': i,\n",
    "    #             't': t,\n",
    "    #             'smiles': deltas[i]['smiles'][t],\n",
    "    #             prop: deltas[i][prop][t],\n",
    "    #             'similarity': deltas[i]['similarity'][t]\n",
    "    #         })\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    df['name'] = name\n",
    "    r.append(df)\n",
    "\n",
    "df_all = pd.concat(r)\n",
    "\n",
    "show_df(df_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064bbac",
   "metadata": {},
   "source": [
    "Plot the Molecular property plogP distribution shifts following the latent flow path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context='paper',\n",
    "    style='ticks',\n",
    "    palette='tab10',\n",
    "    font='serif',\n",
    ")\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "sup_line = df_all.query('t == 999 and name == \"Langevin Dynamics\"')[prop].mean()\n",
    "\n",
    "_df = df_all.query(\n",
    "    '(t % 100 == 0 or t == 999) and name in [\"Random\", \"Gradient Flow\", \"Wave eqn. (spv)\", \"Langevin Dynamics\"]')\n",
    "# _df = df_all.query('t % 100 == 0 or t == 999')\n",
    "\n",
    "# set x-axis limits -10 to 10\n",
    "g = sns.displot(_df, x=prop, hue='t', kind='kde', fill=True, col='name', height=2.5,\n",
    "                col_wrap=5, facet_kws={'sharey': False})\n",
    "g.set_titles('{col_name}')\n",
    "g.set_xlabels('plogp')\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xlim(-12, 5)\n",
    "    #plot vertical line at x=0\n",
    "    ax.axvline(-2.5, color='black', linestyle='--', lw=0.5)\n",
    "g.savefig(f'ChemFlow/figures/optimization/{prop}_spv_kde.pdf')\n",
    "g.savefig(f'ChemFlow/figures/optimization/{prop}_spv_kde.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9d4e8",
   "metadata": {},
   "source": [
    "Table 1 results for the original methods for plogp and qed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "METHOD_CONFIG = {\n",
    "    \"RANDOM\": {\"method_code\": \"random\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"GRADIENT FLOW\": {\"method_code\": \"limo\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"WAVE (SPV)\": {\"method_code\": \"wave_sup\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"WAVE (UNSUP)\": {\"method_code\": \"wave_unsup\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"HJ (SPV)\": {\"method_code\": \"hj_sup\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"HJ (UNSUP)\": {\"method_code\": \"hj_unsup\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"LD\": {\"method_code\": \"fp\", \"suffix\": \"_0.1_relative\"},\n",
    "}\n",
    "\n",
    "PROPERTY_CONFIG = {\n",
    "    \"plogP ↑\": {\"prop_code\": \"plogp\", \"maximize\": True},\n",
    "    \"QED ↑\": {\"prop_code\": \"qed\", \"maximize\": True},\n",
    "    \"ESR1 DOCKING ↓\": {\"prop_code\": \"1err\", \"maximize\": False},\n",
    "    \"ACAA1 DOCKING ↓\": {\"prop_code\": \"2iik\", \"maximize\": False},\n",
    "}\n",
    "\n",
    "ranks = [\"1ST\", \"2ND\", \"3RD\"]\n",
    "table_data = []\n",
    "method_names_ordered = [\"RANDOM\", \"GRADIENT FLOW\", \"WAVE (SPV)\", \"WAVE (UNSUP)\", \"HJ (SPV)\", \"HJ (UNSUP)\", \"LD\"]\n",
    "property_names_ordered = [\"plogP ↑\", \"QED ↑\"]\n",
    "\n",
    "base_path = Path(\"ChemFlow/data/interim/uc_optim\")\n",
    "\n",
    "for method_display_name in method_names_ordered:\n",
    "    config = METHOD_CONFIG[method_display_name]\n",
    "    method_code = config[\"method_code\"]\n",
    "    suffix = config[\"suffix\"]\n",
    "    row_data = {\"METHOD\": method_display_name}\n",
    "\n",
    "    for prop_display_name in property_names_ordered:\n",
    "        prop_info = PROPERTY_CONFIG[prop_display_name]\n",
    "        prop_code = prop_info[\"prop_code\"]\n",
    "        maximize = prop_info[\"maximize\"]\n",
    "\n",
    "        filename = f\"{prop_code}_{method_code}{suffix}.csv\"\n",
    "        filepath = base_path / filename\n",
    "        \n",
    "        df_prop = pd.read_csv(filepath)\n",
    "        \n",
    "        sorted_df = df_prop.sort_values(by=prop_code, ascending=not maximize)\n",
    "        top_3_values = sorted_df[prop_code].head(3).tolist()\n",
    "        \n",
    "        for i, rank in enumerate(ranks):\n",
    "            if i < len(top_3_values):\n",
    "                row_data[(prop_display_name, rank)] = top_3_values[i]\n",
    "            else:\n",
    "                row_data[(prop_display_name, rank)] = float('nan')\n",
    "            \n",
    "    table_data.append(row_data)\n",
    "\n",
    "df_results = pd.DataFrame(table_data)\n",
    "df_results = df_results.set_index(\"METHOD\")\n",
    "df_results.columns = pd.MultiIndex.from_tuples(df_results.columns)\n",
    "\n",
    "column_formats = {}\n",
    "for prop_col_name, _ in df_results.columns:\n",
    "    is_docking = \"DOCKING\" in prop_col_name\n",
    "    is_qed = prop_col_name == \"QED ↑\"\n",
    "    \n",
    "    for rank_col_name in ranks:\n",
    "        if is_docking:\n",
    "            column_formats[(prop_col_name, rank_col_name)] = '{:.2f}'\n",
    "        elif is_qed:\n",
    "            column_formats[(prop_col_name, rank_col_name)] = '{:.3f}'\n",
    "        else: \n",
    "            column_formats[(prop_col_name, rank_col_name)] = '{:.2f}'\n",
    "            \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2188872",
   "metadata": {},
   "source": [
    "Table 3 results for the Similarity-constrained Multi-objective (QED-SA) maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import DataStructs, AllChem\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "METHOD_DISPLAY_TO_CODE = {\n",
    "    \"Random\": {\"code\": \"random\", \"suffix\": \"_0.1_absolute\"},\n",
    "    \"Gradient Flow\": {\"code\": \"limo\", \"suffix\": \"_0.1_absolute\"}, # Suffix was relative\n",
    "    \"Wave (SPV)\": {\"code\": \"wave_sup\", \"suffix\": \"_0.1_absolute\"}, # Suffix was relative\n",
    "    \"Wave (UNSUP)\": {\"code\": \"wave_unsup\", \"suffix\": \"_0.1_absolute\"}, # Suffix was relative\n",
    "    \"HJ (SPV)\": {\"code\": \"hj_sup\", \"suffix\": \"_0.1_absolute\"}, # Suffix was relative\n",
    "    \"HJ (UNSUP)\": {\"code\": \"hj_unsup\", \"suffix\": \"_0.1_absolute\"}, # Suffix was relative\n",
    "    \"LD\": {\"code\": \"fp\", \"suffix\": \"_0.1_absolute\"}, # Suffix was relative\n",
    "}\n",
    "# Corrected suffixes based on user's file list for qed_sa files.\n",
    "\n",
    "SIMILARITY_THRESHOLDS = [0.0, 0.2, 0.4, 0.6]\n",
    "PROPS_TO_OPTIMIZE = [\"qed\", \"sa\"] \n",
    "N_INITIAL_MOLECULES = 800\n",
    "N_OPTIMIZATION_STEPS = 1000 # Max value of 't' is steps-1\n",
    "\n",
    "BASE_OUTPUT_PATH = Path(\"ChemFlow/data/interim/optimization\")\n",
    "ZINC250K_PROPS_PATH = Path(\"ChemFlow/data/interim/props/zinc250k.csv\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_morgan_fingerprint(mol):\n",
    "    return AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
    "\n",
    "def calculate_similarity(smiles1, smiles2):\n",
    "    mol1 = Chem.MolFromSmiles(smiles1)\n",
    "    mol2 = Chem.MolFromSmiles(smiles2)\n",
    "    if mol1 is None or mol2 is None:\n",
    "        return 0.0\n",
    "    fp1 = get_morgan_fingerprint(mol1)\n",
    "    fp2 = get_morgan_fingerprint(mol2)\n",
    "    return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "\n",
    "def scale_property(value, p_min, p_max, target_min=0, target_max=100, maximize=True):\n",
    "    if p_max == p_min: # Avoid division by zero\n",
    "        return target_min if (maximize and value <= p_min) or (not maximize and value >= p_min) else target_max\n",
    "\n",
    "    scaled_val = (value - p_min) / (p_max - p_min)\n",
    "    if not maximize: # For minimization (like SA), invert normalized score\n",
    "        scaled_val = 1 - scaled_val\n",
    "    return scaled_val * (target_max - target_min) + target_min\n",
    "\n",
    "# --- Load and Prepare Initial Data ---\n",
    "df_zinc250k = pd.read_csv(ZINC250K_PROPS_PATH)\n",
    "qed_min_raw, qed_max_raw = df_zinc250k[\"qed\"].min(), df_zinc250k[\"qed\"].max()\n",
    "sa_min_raw, sa_max_raw = df_zinc250k[\"sa\"].min(), df_zinc250k[\"sa\"].max()\n",
    "\n",
    "# Select initial molecules based on the logic in optimization_multi.py\n",
    "df_initial_selection = df_zinc250k[[\"smiles\", \"qed\", \"sa\"]].copy()\n",
    "df_initial_selection[\"qed_scaled_sel\"] = scale_property(df_initial_selection[\"qed\"], qed_min_raw, qed_max_raw, target_max=50, maximize=True)\n",
    "df_initial_selection[\"sa_scaled_sel\"] = scale_property(df_initial_selection[\"sa\"], sa_min_raw, sa_max_raw, target_max=50, maximize=False) # SA is minimized, so maximize=False gives higher score for lower SA\n",
    "df_initial_selection[\"score_sel\"] = df_initial_selection[\"qed_scaled_sel\"] + df_initial_selection[\"sa_scaled_sel\"]\n",
    "df_initial_selection = df_initial_selection.sort_values(by=\"score_sel\", ascending=True).head(N_INITIAL_MOLECULES)\n",
    "\n",
    "initial_molecules_data = {}\n",
    "for i, row in df_initial_selection.iterrows():\n",
    "    initial_molecules_data[len(initial_molecules_data)] = { # Use simple 0-based index matching 'idx'\n",
    "        \"smiles\": row[\"smiles\"],\n",
    "        \"qed_raw\": row[\"qed\"],\n",
    "        \"sa_raw\": row[\"sa\"],\n",
    "        \"qed_scaled_100\": scale_property(row[\"qed\"], qed_min_raw, qed_max_raw, maximize=True),\n",
    "        \"sa_scaled_100\": scale_property(row[\"sa\"], sa_min_raw, sa_max_raw, maximize=False), # Higher scaled_100 is better\n",
    "    }\n",
    "\n",
    "# --- Main Processing Logic ---\n",
    "results_qed = []\n",
    "results_sa = []\n",
    "methods_ordered = list(METHOD_DISPLAY_TO_CODE.keys())\n",
    "\n",
    "for method_display_name in tqdm(methods_ordered, desc=\"Processing methods\"):\n",
    "    method_details = METHOD_DISPLAY_TO_CODE[method_display_name]\n",
    "    method_code = method_details[\"code\"]\n",
    "    suffix = method_details[\"suffix\"]\n",
    "\n",
    "    filename = f\"{PROPS_TO_OPTIMIZE[0]}_{PROPS_TO_OPTIMIZE[1]}_{method_code}{suffix}.csv\"\n",
    "    filepath = BASE_OUTPUT_PATH / filename\n",
    "\n",
    "    row_data_qed = {\"Method\": method_display_name}\n",
    "    row_data_sa = {\"Method\": method_display_name}\n",
    "\n",
    "    if not filepath.exists():\n",
    "        print(f\"File not found: {filepath}, skipping {method_display_name}\")\n",
    "        for sim_thresh in SIMILARITY_THRESHOLDS:\n",
    "            col_name = f\"δ = {sim_thresh:.1f}\"\n",
    "            row_data_qed[col_name] = \"N/A\"\n",
    "            row_data_sa[col_name] = \"N/A\"\n",
    "        results_qed.append(row_data_qed)\n",
    "        results_sa.append(row_data_sa)\n",
    "        continue\n",
    "\n",
    "    df_method_output = pd.read_csv(filepath)\n",
    "\n",
    "    processed_steps = []\n",
    "    for _, gen_row in df_method_output.iterrows():\n",
    "        idx = gen_row[\"idx\"]\n",
    "        if idx not in initial_molecules_data:\n",
    "            continue \n",
    "            \n",
    "        initial_data = initial_molecules_data[idx]\n",
    "        sim = calculate_similarity(gen_row[\"smiles\"], initial_data[\"smiles\"])\n",
    "        \n",
    "        final_qed_scaled_100 = scale_property(gen_row[\"qed\"], qed_min_raw, qed_max_raw, maximize=True)\n",
    "        final_sa_scaled_100 = scale_property(gen_row[\"sa\"], sa_min_raw, sa_max_raw, maximize=False) # Higher is better\n",
    "\n",
    "        delta_qed = final_qed_scaled_100 - initial_data[\"qed_scaled_100\"]\n",
    "        delta_sa = final_sa_scaled_100 - initial_data[\"sa_scaled_100\"]\n",
    "        \n",
    "        processed_steps.append({\n",
    "            \"idx\": idx, \"t\": gen_row[\"t\"], \"sim\": sim,\n",
    "            \"delta_qed\": delta_qed, \"delta_sa\": delta_sa\n",
    "        })\n",
    "    \n",
    "    df_processed_method = pd.DataFrame(processed_steps)\n",
    "\n",
    "    for sim_thresh in SIMILARITY_THRESHOLDS:\n",
    "        col_name = f\"δ = {sim_thresh:.1f}\"\n",
    "        df_sim_filtered = df_processed_method[df_processed_method[\"sim\"] >= sim_thresh]\n",
    "\n",
    "        # QED\n",
    "        if not df_sim_filtered.empty:\n",
    "            best_delta_qed_per_idx = df_sim_filtered.groupby(\"idx\")[\"delta_qed\"].max()\n",
    "        else:\n",
    "            best_delta_qed_per_idx = pd.Series(dtype=float)\n",
    "        \n",
    "        # Ensure all N_INITIAL_MOLECULES are represented, defaulting to -inf if no valid/better step found\n",
    "        all_idx_best_delta_qed = pd.Series(-np.inf, index=range(N_INITIAL_MOLECULES), dtype=float)\n",
    "        all_idx_best_delta_qed.update(best_delta_qed_per_idx)\n",
    "\n",
    "        successful_qed = all_idx_best_delta_qed[all_idx_best_delta_qed > 0]\n",
    "        mean_qed = successful_qed.mean() if not successful_qed.empty else 0.0\n",
    "        std_qed = successful_qed.std() if not successful_qed.empty else 0.0\n",
    "        succ_rate_qed = len(successful_qed) / N_INITIAL_MOLECULES if N_INITIAL_MOLECULES > 0 else 0.0\n",
    "        row_data_qed[col_name] = f\"{mean_qed:.1f} ± {std_qed:.1f} ({succ_rate_qed*100:.1f})\"\n",
    "\n",
    "        # SA\n",
    "        if not df_sim_filtered.empty:\n",
    "            best_delta_sa_per_idx = df_sim_filtered.groupby(\"idx\")[\"delta_sa\"].max()\n",
    "        else:\n",
    "            best_delta_sa_per_idx = pd.Series(dtype=float)\n",
    "\n",
    "        all_idx_best_delta_sa = pd.Series(-np.inf, index=range(N_INITIAL_MOLECULES), dtype=float)\n",
    "        all_idx_best_delta_sa.update(best_delta_sa_per_idx)\n",
    "\n",
    "        successful_sa = all_idx_best_delta_sa[all_idx_best_delta_sa > 0]\n",
    "        mean_sa = successful_sa.mean() if not successful_sa.empty else 0.0\n",
    "        std_sa = successful_sa.std() if not successful_sa.empty else 0.0\n",
    "        succ_rate_sa = len(successful_sa) / N_INITIAL_MOLECULES if N_INITIAL_MOLECULES > 0 else 0.0\n",
    "        row_data_sa[col_name] = f\"{mean_sa:.2f} ± {std_sa:.2f} ({succ_rate_sa*100:.1f})\"\n",
    "        \n",
    "    results_qed.append(row_data_qed)\n",
    "    results_sa.append(row_data_sa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ed612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display Results ---\n",
    "df_table_qed = pd.DataFrame(results_qed).set_index(\"Method\")\n",
    "df_table_sa = pd.DataFrame(results_sa).set_index(\"Method\")\n",
    "\n",
    "# Optional: Display settings\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Title and display\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\"### Multi-objective (QED-SA) Maximization - QED Improvement\"))\n",
    "display(df_table_qed)\n",
    "\n",
    "display(Markdown(\"### Multi-objective (QED-SA) Maximization - SA Improvement (higher scaled score is better)\"))\n",
    "display(df_table_sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89407042",
   "metadata": {},
   "source": [
    "Table 4: Success Rate of traversing latent molecule space to manipulate to manipulate a variety of properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0387376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "METHOD_MAP = {\n",
    "    \"RANDOM-1D\": {\"file_prefix\": \"random_1d\", \"spv\": False},\n",
    "    \"RANDOM\": {\"file_prefix\": \"random\", \"spv\": False},\n",
    " #   \"CHEMSPACE\": {\"file_prefix\": \"chemspace\", \"spv\": False},\n",
    "    \"WAVE (UNSUP)\": {\"file_prefix\": \"unsup_pde\", \"spv\": False},\n",
    "    \"WAVE (SPV)\": {\"file_prefix\": \"sup_pde\", \"spv\": True},\n",
    "    \"HJ (UNSUP)\": {\"file_prefix\": \"unsup_hj\", \"spv\": False},\n",
    "    \"HJ (SPV)\": {\"file_prefix\": \"sup_hj\", \"spv\": True},\n",
    "    \"GF (SPV)\": {\"file_prefix\": \"limo\", \"spv\": True},\n",
    "    \"LD (SPV)\": {\"file_prefix\": \"fp\", \"spv\": True},\n",
    "}\n",
    "\n",
    "PROPERTIES_MAP = {\n",
    "    \"PLOGP (↑)\": \"plogp\",\n",
    "    \"QED (↑)\": \"qed\",\n",
    "    \"SA (↓)\": \"sa\",\n",
    "    \"DRD2 (↑)\": \"drd2\",\n",
    "    \"JNK3 (↑)\": \"jnk3\",\n",
    "    \"GSK3B (↑)\": \"gsk3b\",\n",
    "}\n",
    "\n",
    "N_MOLECULES_SUCCESS_RATE = 1000\n",
    "BASE_SUCCESS_RATE_PATH = Path(f\"ChemFlow/experiments/success_rate\")\n",
    "\n",
    "table_rows = []\n",
    "methods_in_table_order = list(METHOD_MAP.keys())\n",
    "properties_in_table_order = list(PROPERTIES_MAP.keys())\n",
    "\n",
    "for method_display_name in methods_in_table_order:\n",
    "    method_info = METHOD_MAP[method_display_name]\n",
    "    file_prefix_from_map = method_info[\"file_prefix\"]\n",
    "    \n",
    "    row_data = {\"METHOD\": method_display_name}\n",
    "    all_prop_strict_rates = []\n",
    "    all_prop_relaxed_rates = []\n",
    "\n",
    "    for prop_display_name in properties_in_table_order:\n",
    "        prop_code = PROPERTIES_MAP[prop_display_name]\n",
    "        data_file = BASE_SUCCESS_RATE_PATH / f\"{prop_code}_{N_MOLECULES_SUCCESS_RATE}.txt\"\n",
    "        strict_rate = 0.0\n",
    "        relaxed_rate = 0.0\n",
    "\n",
    "        if data_file.exists():\n",
    "            with open(data_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = [p.strip() for p in line.split('|')]\n",
    "                    if len(parts) > 1 and parts[0] == file_prefix_from_map:\n",
    "                        if len(parts) >= 6: \n",
    "                            try:\n",
    "                                strict_val_str = parts[4].split(' ')[-1] if ' ' in parts[4] else parts[4]\n",
    "                                relaxed_val_str = parts[5].split(' ')[-1] if ' ' in parts[5] else parts[5]\n",
    "                                strict_rate = float(strict_val_str) * 100\n",
    "                                relaxed_rate = float(relaxed_val_str) * 100\n",
    "                                break \n",
    "                            except (ValueError, IndexError):\n",
    "                                pass \n",
    "                        \n",
    "        row_data[prop_display_name] = f\"{strict_rate:.2f} / {relaxed_rate:.2f}\"\n",
    "        all_prop_strict_rates.append(strict_rate)\n",
    "        all_prop_relaxed_rates.append(relaxed_rate)\n",
    "\n",
    "    avg_strict = np.mean(all_prop_strict_rates) if all_prop_strict_rates else 0.0\n",
    "    avg_relaxed = np.mean(all_prop_relaxed_rates) if all_prop_relaxed_rates else 0.0\n",
    "    row_data[\"AVERAGE_STRICT_VAL\"] = avg_strict\n",
    "    row_data[\"AVERAGE_RELAXED_VAL\"] = avg_relaxed\n",
    "    row_data[\"AVERAGE_DISPLAY\"] = f\"{avg_strict:.2f} / {avg_relaxed:.2f}\"\n",
    "    table_rows.append(row_data)\n",
    "\n",
    "df_results = pd.DataFrame(table_rows)\n",
    "df_results = df_results.set_index(\"METHOD\") \n",
    "\n",
    "df_results['RANK_STRICT'] = df_results['AVERAGE_STRICT_VAL'].rank(method='min', ascending=False)\n",
    "df_results['RANK_RELAXED'] = df_results['AVERAGE_RELAXED_VAL'].rank(method='min', ascending=False)\n",
    "df_results['AVERAGE_OF_RANKS'] = (df_results['RANK_STRICT'] + df_results['RANK_RELAXED']) / 2\n",
    "df_results['RANKING'] = df_results['AVERAGE_OF_RANKS'].rank(method='min').astype(int)\n",
    "\n",
    "cols_display = ['RANKING', 'AVERAGE_DISPLAY'] + properties_in_table_order\n",
    "df_final_display = df_results.reindex(methods_in_table_order)[cols_display]\n",
    "df_final_display.rename(columns={'AVERAGE_DISPLAY': 'AVERAGE'}, inplace=True)\n",
    "df_final_display.index.name = None\n",
    "\n",
    "styled_table = df_final_display.style.set_caption(\"Table 4: Success Rate of Traversing Latent Molecule Space\").set_table_styles(\n",
    "    [{'selector': 'caption',\n",
    "      'props': [('color', 'black'),\n",
    "                ('font-size', '16px'),\n",
    "                ('font-weight', 'bold'),\n",
    "                ('text-align', 'center')]}\n",
    "    ]\n",
    ").format(precision=2)\n",
    "\n",
    "display(Markdown(\"### Table 4: Success Rate of Traversing Latent Molecule Space\"))\n",
    "display(styled_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa25937",
   "metadata": {},
   "source": [
    "Up to here we have dealt with the inital implementation. Moving now to extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469b66",
   "metadata": {},
   "source": [
    "Perform the random walkd and the neyghboring search for unsuperivsed scenario using MolGenTransformers for qed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db81346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ChemFlow/extend/MolgenRandomWalkSimilarityConstrained.py qed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811455c0",
   "metadata": {},
   "source": [
    "Same for plogp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ChemFlow/extend/MolgenRandomWalkSimilarityConstrained.py plogp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad4f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def get_top_n_property_values(df_for_selection: pd.DataFrame, property_col: str, n_top: int, ascending_order: bool):\n",
    "    if df_for_selection.empty or property_col not in df_for_selection.columns:\n",
    "        return [np.nan] * n_top\n",
    "    \n",
    "    sorted_df = df_for_selection.sort_values(by=property_col, ascending=ascending_order, na_position='last')\n",
    "    top_values = sorted_df[property_col].head(n_top).tolist()\n",
    "    \n",
    "    if len(top_values) < n_top:\n",
    "        top_values.extend([np.nan] * (n_top - len(top_values)))\n",
    "    return top_values\n",
    "\n",
    "def analyze_method_results(\n",
    "    plogp_csv_path_str: str,\n",
    "    qed_csv_path_str: str,\n",
    "    blacklist_smiles: set\n",
    "):\n",
    "    plogp_path = Path(plogp_csv_path_str)\n",
    "    qed_path   = Path(qed_csv_path_str)\n",
    "    \n",
    "    default_results = {\n",
    "        ('plogP ↑', '1ST'): np.nan, ('plogP ↑', '2ND'): np.nan, ('plogP ↑', '3RD'): np.nan,\n",
    "        ('QED ↑',  '1ST'): np.nan, ('QED ↑',  '2ND'): np.nan, ('QED ↑',  '3RD'): np.nan,\n",
    "    }\n",
    "\n",
    "    if not plogp_path.is_file() or not qed_path.is_file():\n",
    "        return default_results\n",
    "\n",
    "    df_plogp = pd.read_csv(plogp_path).dropna(subset=['smiles','plogp'])\n",
    "    df_qed   = pd.read_csv(qed_path)  .dropna(subset=['smiles','qed'])\n",
    "    df_plogp['smiles'] = df_plogp['smiles'].astype(str)\n",
    "    df_qed['smiles']   = df_qed['smiles'].astype(str)\n",
    "\n",
    "    df_plogp = df_plogp[~df_plogp['smiles'].isin(blacklist_smiles)].drop_duplicates('smiles', keep='first')\n",
    "    df_qed   = df_qed[~df_qed['smiles'].isin(blacklist_smiles)].drop_duplicates('smiles', keep='first')\n",
    "    if df_plogp.empty or df_qed.empty:\n",
    "        return default_results\n",
    "\n",
    "    top_plogp = get_top_n_property_values(df_plogp, 'plogp', 3, False)\n",
    "    top_qed   = get_top_n_property_values(df_qed,   'qed',   3, False)\n",
    "    \n",
    "    return {\n",
    "        ('plogP ↑', '1ST'): top_plogp[0], ('plogP ↑', '2ND'): top_plogp[1], ('plogP ↑', '3RD'): top_plogp[2],\n",
    "        ('QED ↑',  '1ST'): top_qed[0],  ('QED ↑',  '2ND'): top_qed[1],  ('QED ↑',  '3RD'): top_qed[2],\n",
    "    }\n",
    "\n",
    "blacklist_file_path = \"ChemFlow/data/interim/props/zinc250k.csv\"\n",
    "\n",
    "input_csv_for_random_walk = \"ChemFlow/extend/optimization_results_molgen_truncated_100steps/plogp_random_walk_s2.0_t100_n800.csv\"\n",
    "input_csv_for_neighbor_search = \"ChemFlow/extend/optimization_results_molgen_truncated_100steps/plogp_neighboring_search_nv10_r20.0_res0.05_n800.csv\"\n",
    "\n",
    "loaded_blacklist_smiles = set()\n",
    "blacklist_path_obj = Path(blacklist_file_path)\n",
    "if blacklist_path_obj.is_file():\n",
    "    df_blacklist_data = pd.read_csv(blacklist_path_obj)\n",
    "    if 'smiles' in df_blacklist_data.columns:\n",
    "        loaded_blacklist_smiles = set(df_blacklist_data['smiles'].astype(str).dropna())\n",
    "\n",
    "analysis_rows = []\n",
    "\n",
    "method_configurations = [\n",
    "    {\n",
    "      \"name\": \"random_walk_transformer\",\n",
    "      \"plogp_path\": input_csv_for_random_walk,\n",
    "      \"qed_path\":   input_csv_for_random_walk.replace(\"plogp\", \"qed\")\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"neighbor_search_transformer\",\n",
    "      \"plogp_path\": input_csv_for_neighbor_search,\n",
    "      \"qed_path\":   input_csv_for_neighbor_search.replace(\"plogp\", \"qed\")\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in method_configurations:\n",
    "    current_method_properties = analyze_method_results(\n",
    "        config[\"plogp_path\"],\n",
    "        config[\"qed_path\"],\n",
    "        loaded_blacklist_smiles\n",
    "    )\n",
    "    current_method_row = {'METHOD': config[\"name\"], **current_method_properties}\n",
    "    analysis_rows.append(current_method_row)\n",
    "\n",
    "output_dataframe = pd.DataFrame(analysis_rows)\n",
    "\n",
    "if not output_dataframe.empty:\n",
    "    output_dataframe = output_dataframe.set_index('METHOD')\n",
    "    \n",
    "    column_multi_index = pd.MultiIndex.from_tuples([\n",
    "        ('plogP ↑', '1ST'), ('plogP ↑', '2ND'), ('plogP ↑', '3RD'),\n",
    "        ('QED ↑',  '1ST'), ('QED ↑',  '2ND'), ('QED ↑',  '3RD')\n",
    "    ])\n",
    "    output_dataframe = output_dataframe.reindex(columns=column_multi_index)\n",
    "else:\n",
    "    column_multi_index = pd.MultiIndex.from_product(\n",
    "        [['plogP ↑', 'QED ↑'], ['1ST', '2ND', '3RD']],\n",
    "    )\n",
    "    output_dataframe = pd.DataFrame(columns=column_multi_index, index=pd.Index([], name='METHOD'))\n",
    "    \n",
    "output_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe58ae",
   "metadata": {},
   "source": [
    "These are some approaches to train a neural ode that did not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc02e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python experiments/train_neural_ode.py --prop_name plogp --data_name zmc --is_supervised --epochs 40 --output_dir_base checkpoints/neural_ode --model_params.latent_dim 1024 --model_params.learning_rate 1e-4 --model_params.integration_time_step 0.2 --model_params.num_loss_integration_steps 5 --model_params.flow_reg_lambda 0.01 --data_params.batch_size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python experiments/train_neural_ode.py     --prop_name qed     --data_name zmc     --is_supervised     --epochs 40     --output_dir_base checkpoints/neural_ode     --model_params.latent_dim 1024     --model_params.learning_rate 1e-4     --model_params.integration_time_step 0.2     --model_params.num_loss_integration_st eps 5     --model_params.flow_reg_lambda 0.01     --data_params.batch_size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd02dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python experiments/optimization/uc_optim.py     --prop plogp     --method neural_ode     --data_name zmc     --n 100000     --steps 10     --batch_size 1000     --step_size 0.1     --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4417798",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python experiments/optimization/uc_optim.py     --prop qed     --method neural_ode     --data_name zmc     --n 100000     --steps 10     --batch_size 1000     --step_size 0.1     --seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87ae0e",
   "metadata": {},
   "source": [
    "Present the results for qed and plogp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "METHOD_CONFIG = {\n",
    "    \"RANDOM\": {\"method_code\": \"random\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"GRADIENT FLOW\": {\"method_code\": \"limo\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"WAVE (SPV)\": {\"method_code\": \"wave_sup\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"WAVE (UNSUP)\": {\"method_code\": \"wave_unsup\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"HJ (SPV)\": {\"method_code\": \"hj_sup\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"HJ (UNSUP)\": {\"method_code\": \"hj_unsup\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"LD\": {\"method_code\": \"fp\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"ODE\": {\"method_code\": \"neural_ode\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"LATENT STEPPER\": {\"method_code\": \"latent_stepper\", \"suffix\": \"_0.1_relative\"},\n",
    "    \"hybrid_ld_hj\": {\"method_code\": \"hybrid_sup_unsup\", \"suffix\": \"_0.1_relative\"},\n",
    "}\n",
    "\n",
    "PROPERTY_CONFIG = {\n",
    "    \"plogP ↑\": {\"prop_code\": \"plogp\", \"maximize\": True},\n",
    "    \"QED ↑\": {\"prop_code\": \"qed\", \"maximize\": True},\n",
    "    \"ESR1 DOCKING ↓\": {\"prop_code\": \"1err\", \"maximize\": False},\n",
    "    \"ACAA1 DOCKING ↓\": {\"prop_code\": \"2iik\", \"maximize\": False},\n",
    "}\n",
    "\n",
    "ranks = [\"1ST\", \"2ND\", \"3RD\"]\n",
    "table_data = []\n",
    "method_names_ordered = [\n",
    "    \"RANDOM\", \"GRADIENT FLOW\", \"WAVE (SPV)\", \"WAVE (UNSUP)\",\n",
    "    \"HJ (SPV)\", \"HJ (UNSUP)\", \"LD\", \"ODE\",\n",
    "    \"LATENT STEPPER\", \"hybrid_ld_hj\"\n",
    "]\n",
    "property_names_ordered = [\"plogP ↑\", \"QED ↑\"]\n",
    "\n",
    "base_path = Path(\"ChemFlow/data/interim/uc_optim\")\n",
    "\n",
    "for method_display_name in method_names_ordered:\n",
    "    config = METHOD_CONFIG[method_display_name]\n",
    "    method_code = config[\"method_code\"]\n",
    "    suffix = config[\"suffix\"]\n",
    "    row_data = {\"METHOD\": method_display_name}\n",
    "\n",
    "    for prop_display_name in property_names_ordered:\n",
    "        prop_info = PROPERTY_CONFIG[prop_display_name]\n",
    "        prop_code = prop_info[\"prop_code\"]\n",
    "        maximize = prop_info[\"maximize\"]\n",
    "\n",
    "        filename = f\"{prop_code}_{method_code}{suffix}.csv\"\n",
    "        filepath = base_path / filename\n",
    "\n",
    "        # try to read the file, otherwise fill with NaNs\n",
    "        try:\n",
    "            df_prop = pd.read_csv(filepath)\n",
    "            sorted_df = df_prop.sort_values(by=prop_code, ascending=not maximize)\n",
    "            top_3_values = sorted_df[prop_code].head(3).tolist()\n",
    "        except FileNotFoundError:\n",
    "            top_3_values = [np.nan, np.nan, np.nan]\n",
    "\n",
    "        for i, rank in enumerate(ranks):\n",
    "            row_data[(prop_display_name, rank)] = top_3_values[i]\n",
    "\n",
    "    table_data.append(row_data)\n",
    "    # Add analysis_rows results to table_data\n",
    "for method_name, row in output_dataframe.iterrows():\n",
    "    method_row = {\"METHOD\": method_name}\n",
    "    \n",
    "    # Add all property-rank pairs from the row to method_row\n",
    "    for (prop_name, rank), value in row.items():\n",
    "        method_row[(prop_name, rank)] = value\n",
    "    \n",
    "    table_data.append(method_row)\n",
    "df_results = pd.DataFrame(table_data)\n",
    "#concate output_dataframe and table_data to df_results\n",
    "#df1_for_concat = output_dataframe.reset_index()\n",
    "#df2_for_concat = pd.DataFrame(table_data)\n",
    "#df_results = pd.concat([df1_for_concat, df2_for_concat], axis=0, ignore_index=True)\n",
    "df_results = df_results.set_index(\"METHOD\")\n",
    "df_results.columns = pd.MultiIndex.from_tuples(df_results.columns)\n",
    "\n",
    "# formatting strings per column\n",
    "column_formats = {}\n",
    "for prop_col_name, _ in df_results.columns:\n",
    "    is_docking = \"DOCKING\" in prop_col_name\n",
    "    is_qed = prop_col_name == \"QED ↑\"\n",
    "    for rank_col_name in ranks:\n",
    "        if is_docking:\n",
    "            column_formats[(prop_col_name, rank_col_name)] = \"{:.2f}\"\n",
    "        elif is_qed:\n",
    "            column_formats[(prop_col_name, rank_col_name)] = \"{:.3f}\"\n",
    "        else:\n",
    "            column_formats[(prop_col_name, rank_col_name)] = \"{:.2f}\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "# --- NEW: use Styler to format and bold the best values ---\n",
    "styler = (\n",
    "    df_results\n",
    "    .style\n",
    "    .format(column_formats)\n",
    "    .highlight_max(axis=0, props=\"font-weight:bold\")\n",
    ")\n",
    "\n",
    "styler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e764b06",
   "metadata": {},
   "source": [
    "its time to measure the 'real' properties of our molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449da79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rdkit import Chem\n",
    "\n",
    "# --- The Normalization Function ---\n",
    "# This function takes a SMILES string, tries to read it,\n",
    "# removes explicit hydrogens, and returns a clean, canonical SMILES string.\n",
    "# If it fails at any point, it returns None.\n",
    "def normalize_and_validate_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        Chem.RemoveHs(mol)\n",
    "        return Chem.MolToSmiles(mol, isomericSmiles=True)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- Main Data Loading and Processing Loop ---\n",
    "file_paths = [\n",
    "    \"ChemFlow/data/interim/uc_optim/plogp_random_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/qed_random_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/plogp_limo_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/qed_limo_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/plogp_wave_sup_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/qed_wave_sup_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/plogp_wave_unsup_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/qed_wave_unsup_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/plogp_hj_sup_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/qed_hj_sup_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/plogp_hj_unsup_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/qed_hj_unsup_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/plogp_fp_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/qed_fp_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/plogp_neural_ode_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/qed_neural_ode_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/plogp_latent_stepper_0.1_relative.csv\",\n",
    "    \"ChemFlow/data/interim/uc_optim/plogp_hybrid_sup_unsup_0.1_relative.csv\",\n",
    "    \"ChemFlow/extend/optimization_results_molgen_truncated_100steps/plogp_random_walk_s2.0_t100_n800.csv\",\n",
    "    \"ChemFlow/extend/optimization_results_molgen_truncated_100steps/qed_random_walk_s2.0_t100_n800.csv\",\n",
    "    \"ChemFlow/extend/optimization_results_molgen_truncated_100steps/plogp_neighboring_search_nv10_r20.0_res0.05_n800.csv\",\n",
    "    \"ChemFlow/extend/optimization_results_molgen_truncated_100steps/qed_neighboring_search_nv10_r20.0_res0.05_n800.csv\"\n",
    "]\n",
    "\n",
    "top_molecules_list = []\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['smiles'] = df['smiles'].str.strip()\n",
    "    filename = os.path.basename(file_path)\n",
    "    if 'qed' in filename: score_column = 'qed'\n",
    "    else: score_column = 'plogp'\n",
    "    optimization_method = filename.replace('.csv', '').replace('_0.1_relative', '')\n",
    "    top_3_df = df.sort_values(by=score_column, ascending=False).head(3).copy()\n",
    "    top_3_df['optimization_method'] = optimization_method\n",
    "    top_3_df['score_type'] = score_column\n",
    "    top_3_df['original_score'] = top_3_df[score_column]\n",
    "    top_molecules_list.append(top_3_df)\n",
    "\n",
    "master_df = pd.concat(top_molecules_list, ignore_index=True)\n",
    "\n",
    "# --- Apply the new normalization and validation step ---\n",
    "master_df['clean_smiles'] = master_df['smiles'].apply(normalize_and_validate_smiles)\n",
    "\n",
    "# Create the final, clean DataFrame by dropping any rows where normalization failed\n",
    "valid_master_df = master_df.dropna(subset=['clean_smiles']).copy()\n",
    "\n",
    "# Create the RDKit molecule objects from the CLEANED smiles\n",
    "valid_master_df['rdkit_mol'] = valid_master_df['clean_smiles'].apply(Chem.MolFromSmiles)\n",
    "\n",
    "print(f\"Total molecules loaded: {len(master_df)}\")\n",
    "print(f\"Total VALID and NORMALIZED molecules: {len(valid_master_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195f624",
   "metadata": {},
   "source": [
    "Visualize the molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Final Grid Generation in Chunks ---\n",
    "\n",
    "# Use the fully validated and normalized data\n",
    "mols = list(valid_master_df['rdkit_mol'])\n",
    "legends = [f\"{row['optimization_method']}\\nScore: {row['original_score']:.2f}\" for index, row in valid_master_df.iterrows()]\n",
    "\n",
    "# The chunk size can be adjusted for layout preference. 12 is a good number for a wide display.\n",
    "chunk_size = 12\n",
    "print(f\"--- Generating Final Grid Image in Chunks of {chunk_size} ---\")\n",
    "\n",
    "for i in range(0, len(mols), chunk_size):\n",
    "    start_index = i\n",
    "    end_index = i + chunk_size\n",
    "    \n",
    "    mol_chunk = mols[start_index:end_index]\n",
    "    legend_chunk = legends[start_index:end_index]\n",
    "    \n",
    "    try:\n",
    "        # Generate the image for the current chunk\n",
    "        img = Draw.MolsToGridImage(mol_chunk, molsPerRow=6, subImgSize=(250, 250), legends=legend_chunk)\n",
    "        \n",
    "        # Display the image for this chunk\n",
    "        display(img)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # This part should not be triggered now, but it's good practice to keep it\n",
    "        print(f\"\\n--- SKIPPING a problematic chunk (indices {start_index}-{end_index-1}) due to an error ---\")\n",
    "        print(f\"Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n--- Full Grid Generation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "mol_objects = [Chem.MolFromSmiles(s) for s in master_df['smiles']]\n",
    "\n",
    "master_df['MolWt'] = [Descriptors.MolWt(m) for m in mol_objects]\n",
    "master_df['LogP'] = [Descriptors.MolLogP(m) for m in mol_objects]\n",
    "master_df['NumHDonors'] = [Descriptors.NumHDonors(m) for m in mol_objects]\n",
    "master_df['NumHAcceptors'] = [Descriptors.NumHAcceptors(m) for m in mol_objects]\n",
    "master_df['NumRotatableBonds'] = [Descriptors.NumRotatableBonds(m) for m in mol_objects]\n",
    "# set display to display all rows \n",
    "pd.set_option('display.max_rows', None)\n",
    "display(master_df[['optimization_method', 'original_score', 'MolWt', 'LogP', 'NumHDonors', 'NumHAcceptors']].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828572e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.FilterCatalog import FilterCatalog, FilterCatalogParams\n",
    "\n",
    "params = FilterCatalogParams()\n",
    "params.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS)\n",
    "catalog = FilterCatalog(params)\n",
    "\n",
    "def count_lipinski_violations(mol):\n",
    "    violations = 0\n",
    "    if Descriptors.MolWt(mol) > 500:\n",
    "        violations += 1\n",
    "    if Descriptors.MolLogP(mol) > 5:\n",
    "        violations += 1\n",
    "    if Descriptors.NumHDonors(mol) > 5:\n",
    "        violations += 1\n",
    "    if Descriptors.NumHAcceptors(mol) > 10:\n",
    "        violations += 1\n",
    "    return violations\n",
    "\n",
    "def has_pains(mol):\n",
    "    return catalog.HasMatch(mol)\n",
    "\n",
    "master_df['Lipinski_Violations'] = [count_lipinski_violations(m) for m in mol_objects]\n",
    "master_df['Is_PAINS'] = [has_pains(m) for m in mol_objects]\n",
    "master_df['TPSA'] = [Descriptors.TPSA(m) for m in mol_objects]\n",
    "\n",
    "pd.set_option('display.max_rows', len(master_df))\n",
    "display(master_df[['optimization_method', 'original_score', 'Lipinski_Violations', 'Is_PAINS', 'TPSA']].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b4dd4c",
   "metadata": {},
   "source": [
    "Donwload the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e04d30",
   "metadata": {},
   "source": [
    "3D Ligan filed for docking preperation of the molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv(\"master_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand_dir = \"ligand_files_3d\"\n",
    "os.makedirs(ligand_dir, exist_ok=True)\n",
    "\n",
    "for index, row in master_df.iterrows():\n",
    "    mol = Chem.MolFromSmiles(row['smiles'])\n",
    "    mol_h = Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(mol_h, randomSeed=42)\n",
    "    try:\n",
    "        AllChem.MMFFOptimizeMolecule(mol_h)\n",
    "    except:\n",
    "        continue\n",
    "    ligand_filename = os.path.join(ligand_dir, f\"ligand_{index}.sdf\")\n",
    "    writer = Chem.SDWriter(ligand_filename)\n",
    "    writer.write(mol_h)\n",
    "    writer.close()\n",
    "\n",
    "print(f\"Generated 3D structures for all {len(master_df)} ligands in '{ligand_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccadc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define the optimization methods from the last 4 files\n",
    "last_four_methods = [\n",
    "    \"plogp_random_walk_s2.0_t100_n800\",\n",
    "    \"qed_random_walk_s2.0_t100_n800\",\n",
    "    \"plogp_neighboring_search_nv10_r20.0_res0.05_n800\",\n",
    "    \"qed_neighboring_search_nv10_r20.0_res0.05_n800\"\n",
    "]\n",
    "\n",
    "# Filter the master DataFrame to get only the molecules from these methods\n",
    "subset_df = master_df[master_df['optimization_method'].isin(last_four_methods)].reset_index()\n",
    "\n",
    "# Setup the grid for 12 molecules (3 rows, 4 columns)\n",
    "rows = 3\n",
    "cols = 4\n",
    "view = py3Dmol.view(width=1200, height=600, viewergrid=(rows, cols))\n",
    "\n",
    "for i, row_data in subset_df.iterrows():\n",
    "    # The original index is what we used to name the file\n",
    "    original_index = row_data['index']\n",
    "    sdf_file = os.path.join(ligand_dir, f\"ligand_{original_index}.sdf\")\n",
    "    \n",
    "    with open(sdf_file, 'r') as f:\n",
    "        sdf_content = f.read()\n",
    "    \n",
    "    # Get metadata for the label\n",
    "    opt_method = row_data['optimization_method']\n",
    "    score = row_data['original_score']\n",
    "    label = f\"L{original_index}: {opt_method[:20]}... ({score:.2f})\"\n",
    "    \n",
    "    viewer_row = i // cols\n",
    "    viewer_col = i % cols\n",
    "    \n",
    "    view.addModel(sdf_content, 'sdf', viewer=(viewer_row, viewer_col))\n",
    "    view.setStyle({'stick': {}}, viewer=(viewer_row, viewer_col))\n",
    "    #view.addLabel(label, {'fontColor':'black', 'backgroundColor': 'lightgray', 'fontSize': 12}, viewer=(viewer_row, viewer_col))\n",
    "    view.zoomTo(viewer=(viewer_row, viewer_col))\n",
    "\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aadbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "# import wget\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from ChemFlow.src.vae import load_vae_demo,load_vae\n",
    "from ChemFlow.src.pinn.pde import load_wavepde\n",
    "from ChemFlow.src.pinn import PropGenerator, VAEGenerator\n",
    "from ChemFlow.src.predictor import Predictor\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from tdc import Oracle\n",
    "\n",
    "SmilesScorer = Callable[[str | list[str]], float | list[float]]\n",
    "\n",
    "smiles2sa: SmilesScorer = Oracle(name=\"SA\")\n",
    "smiles2qed: SmilesScorer = Oracle(name=\"QED\")\n",
    "smiles2plogp: SmilesScorer = Oracle(name=\"LogP\")  \n",
    "smiles2gsk3b: SmilesScorer = Oracle(name=\"GSK3B\")\n",
    "smiles2jnk3: SmilesScorer = Oracle(name=\"JNK3\")\n",
    "smiles2drd2: SmilesScorer = Oracle(name=\"DRD2\")\n",
    "\n",
    "PROP_FN = {\n",
    "    \"sa\": smiles2sa,\n",
    "    \"qed\": smiles2qed,\n",
    "    \"plogp\": smiles2plogp,\n",
    "    \"gsk3b\": smiles2gsk3b,\n",
    "    \"jnk3\": smiles2jnk3,\n",
    "    \"drd2\": smiles2drd2,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != \"cuda\":\n",
    "    raise ValueError(\"This notebook requires a GPU\")\n",
    "\n",
    "def normalize(x, step_size=None, relative=False):\n",
    "    if step_size is None:\n",
    "        return x\n",
    "    if relative:\n",
    "        return x * step_size\n",
    "    try:\n",
    "        return x / torch.norm(x, dim=-1, keepdim=True) * step_size\n",
    "    except AttributeError:\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model(model_name, prop_name, dm, vae):\n",
    "    \"\"\"Gets model by name.\"\"\"\n",
    "\n",
    "    if model_name == 'Random-1D':\n",
    "        rand_1d_u_z = torch.zeros(1024).to(device)\n",
    "        rand_1d_u_z[torch.randint(0, 1024, (1,))] = random.choice([-1, 1])\n",
    "        return rand_1d_u_z, None\n",
    "    elif model_name == 'Random':\n",
    "        rand_u_z = torch.randn(1024, device=device)\n",
    "        return rand_u_z, None\n",
    "\n",
    "\n",
    "    predictor = Predictor(dm.max_len * dm.vocab_size)\n",
    "\n",
    "    prop_name = prop_name.lower()\n",
    "    predictor.load_state_dict(\n",
    "        torch.load(\n",
    "            f\"ChemFlow/checkpoints/prop_predictor/{prop_name}/checkpoint.pt\", map_location=device\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # print('loaded predictor')\n",
    "    for p in predictor.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    sup_generator = PropGenerator(vae, predictor).to(device)\n",
    "    for p in sup_generator.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    unsup_generator = VAEGenerator(vae).to(device)\n",
    "\n",
    "    unsup_pde = load_wavepde(\n",
    "        checkpoint=f\"ChemFlow/checkpoints/wavepde/zmc/checkpoint.pt\",\n",
    "        generator=unsup_generator,\n",
    "        k=10,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "    # print('loaded unsup pde')\n",
    "    wave_idx_map = {\"plogp\": 0, \"sa\": 6, \"qed\": 4, \"drd2\": 2, \"jnk3\": 0, \"gsk3b\": 0}\n",
    "    unsup_pde_idx = wave_idx_map[prop_name]\n",
    "    for p in unsup_pde.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    sup_pde = load_wavepde(\n",
    "        checkpoint=f\"ChemFlow/checkpoints/wavepde_prop/zmc/{prop_name}/checkpoint.pt\",\n",
    "        generator=sup_generator,\n",
    "        k=1,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "    # print('loaded sup pde')\n",
    "    sup_pde_idx = 0\n",
    "    for p in sup_pde.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    if model_name == 'Wave (unsupervised)':\n",
    "        return unsup_pde, unsup_pde_idx\n",
    "    elif model_name == 'Wave (supervised)':\n",
    "        return sup_pde, sup_pde_idx\n",
    "    elif model_name == 'Gradient Flow' or model_name == 'Langevin Dynamics':\n",
    "        return sup_pde, None,\n",
    "\n",
    "    unsup_hj = load_wavepde(\n",
    "        checkpoint=f\"ChemFlow/checkpoints/hjpde/zmc/checkpoint.pt\",\n",
    "        generator=unsup_generator,\n",
    "        k=10,\n",
    "        device=device,\n",
    "    )\n",
    "    # print('loaded unsup hj')\n",
    "    hj_idx_map = {\"plogp\": 5, \"sa\": 1, \"qed\": 9, \"drd2\": 6, \"jnk3\": 5, \"gsk3b\": 5}\n",
    "    unsup_hj_idx = hj_idx_map[prop_name]\n",
    "    for p in unsup_hj.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    sup_hj = load_wavepde(\n",
    "        checkpoint=f\"ChemFlow/checkpoints/hjpde_prop/zmc/{prop_name}/checkpoint.pt\",\n",
    "        generator=sup_generator,\n",
    "        k=1,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # print('loaded sup hj')\n",
    "    sup_hj_idx = 0\n",
    "    for p in sup_hj.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "\n",
    "    if model_name == 'HJ (unsupervised)':\n",
    "        return unsup_hj, unsup_hj_idx\n",
    "    elif model_name == 'HJ (supervised)':\n",
    "        return sup_hj, sup_hj_idx\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model name: {model_name}\")\n",
    "\n",
    "\n",
    "\n",
    "def sample(num_samples=1):\n",
    "    \"\"\"Samples latent codes.\"\"\"\n",
    "    z0 = torch.randn((num_samples, 1024), device=device)\n",
    "    return z0\n",
    "\n",
    "\n",
    "def synthesize_original(_code, _vae, _dm, _device):\n",
    "    z = _code.clone()\n",
    "    z = torch.from_numpy(z.detach().cpu().numpy()).float().to(_device)\n",
    "    x = _vae.decode(z).exp()\n",
    "    smiles = _dm.decode(x)[0]\n",
    "    plogp_val = PROP_FN['plogp'](smiles)\n",
    "    sa_val = PROP_FN['sa'](smiles)\n",
    "    qed_val = PROP_FN['qed'](smiles)\n",
    "    drd2_val = PROP_FN['drd2'](smiles)\n",
    "    jnk3_val = PROP_FN['jnk3'](smiles)\n",
    "    gsk3b_val = PROP_FN['gsk3b'](smiles)\n",
    "    label = f'{smiles} \\n plogp⬆ {plogp_val:.3f} \\n sa⬇ {sa_val:.3f} \\n qed⬆ {qed_val:.3f} \\n drd2⬆ {drd2_val:.3f} \\n jnk3⬆ {jnk3_val:.3f} \\n gsk3b⬆ {gsk3b_val:.3f}'\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    img = Draw.MolToImage(mol, legends=label)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "def synthesize_final(_code, _vae, _dm, _device):\n",
    "    z = torch.from_numpy(_code.detach().cpu().numpy()).float().to(_device)\n",
    "    x = _vae.decode(z).exp()\n",
    "    smiles = _dm.decode(x)[0]\n",
    "    plogp_val = PROP_FN['plogp'](smiles)\n",
    "    sa_val = PROP_FN['sa'](smiles)\n",
    "    qed_val = PROP_FN['qed'](smiles)\n",
    "    drd2_val = PROP_FN['drd2'](smiles)\n",
    "    jnk3_val = PROP_FN['jnk3'](smiles)\n",
    "    gsk3b_val = PROP_FN['gsk3b'](smiles)\n",
    "    label = f'{smiles} \\n plogp⬆ {plogp_val:.3f} \\n sa⬇ {sa_val:.3f} \\n qed⬆ {qed_val:.3f} \\n drd2⬆ {drd2_val:.3f} \\n jnk3⬆ {jnk3_val:.3f} \\n gsk3b⬆ {gsk3b_val:.3f}'\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    img = Draw.MolToImage(mol, legends=label)\n",
    "    return img, label\n",
    "\n",
    "properties = [\"plogp\", \"sa\", \"qed\", \"drd2\", \"jnk3\", \"gsk3b\"]\n",
    "\n",
    "dm, vae = load_vae(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0171f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Wave (unsupervised)\" #@param ['Wave (unsupervised)', 'Wave (supervised)', 'HJ (unsupervised)', 'HJ (supervised)']\n",
    "\n",
    "model_lst = []\n",
    "for prop_name in properties:\n",
    "    print(f\"Loading model for {model_name} with property {prop_name}\")\n",
    "    model = get_model(model_name, prop_name, dm, vae)\n",
    "    model_lst.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b04a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_codes = sample(num_samples=1)\n",
    "z = base_codes.clone()\n",
    "\n",
    "original_image, original_label = synthesize_original(z, vae, dm, device)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(original_image)\n",
    "plt.title('Initial Molecule\\n'+original_label)\n",
    "plt.axis('off')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35c4028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max QED value in the file: 0.7732360353755409\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"ChemFlow/extend/optimization_results_molgen_guided_single_gpu/guided_single_gpu_qed_t100_n100.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "#print max qed value\n",
    "max_qed = df['qed'].max()\n",
    "print(f\"Max QED value in the file: {max_qed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = base_codes.clone()\n",
    "\n",
    "num_semantics = 6\n",
    "\n",
    "#@markdown plogP ⬆\n",
    "plogp_steps = 4 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
    "#@markdown SA ⬇\n",
    "sa_steps  = 0 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
    "#@markdown QED ⬆\n",
    "qed_steps  = 2 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
    "#@markdown DRD2 ⬆\n",
    "drd2_steps  = 0 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
    "#@markdown JNK3 ⬆\n",
    "jnk3_steps  = 0 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
    "#@markdown GSK3B  ⬆\n",
    "gsk3b_steps = 0 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
    "\n",
    "steps = {0 : plogp_steps, 1 : sa_steps, 2 : qed_steps, 3 : drd2_steps, 4 : jnk3_steps, 5 : gsk3b_steps}\n",
    "\n",
    "step_size = 0.05\n",
    "relative = True\n",
    "\n",
    "for sem_idx, step in steps.items():\n",
    "    if step < 0:\n",
    "        total_steps = -step\n",
    "    else:\n",
    "        total_steps = step\n",
    "    for t in range(total_steps):\n",
    "        model = model_lst[sem_idx]\n",
    "        if t == 0:\n",
    "            u_z = 0\n",
    "        else:\n",
    "            if model_name == \"Random-1D\":\n",
    "                u_z = model[0]\n",
    "                u_z = normalize(u_z, step_size, relative)\n",
    "            elif model_name == \"Random\":\n",
    "                u_z = model[0]\n",
    "                u_z = normalize(u_z, step_size, relative)\n",
    "            elif model_name == \"Wave (unsupervised)\":\n",
    "                u, u_z = model[0].inference(model[1], z, t % 10)\n",
    "                u_z = normalize(u_z, step_size, relative)\n",
    "            elif model_name == \"Wave (supervised)\":\n",
    "                u, u_z = model[0].inference(model[1], z, t % 10)\n",
    "                u_z = normalize(u_z, step_size, relative)\n",
    "            elif model_name == \"HJ (unsupervised)\":\n",
    "                u, u_z = model[0].inference(model[1], z, t % 10)\n",
    "                u_z = normalize(u_z, step_size, relative)\n",
    "            elif model_name == \"HJ (supervised)\":\n",
    "                u, u_z = model[0].inference(model[1], z, t % 10)\n",
    "                u_z = normalize(u_z, step_size, relative)\n",
    "            elif model_name == \"Langevin Dynamics\":\n",
    "                assert relative and step_size is not None\n",
    "                z = z.detach().requires_grad_(True)\n",
    "                u_z = torch.autograd.grad(model[0].generator(z).sum(), z)[0]\n",
    "                u_z = u_z * step_size + torch.randn_like(u_z) * np.sqrt(2 * step_size) * 0.01\n",
    "            elif model_name == \"Gradient Flow\":\n",
    "                z = z.detach().requires_grad_(True)\n",
    "                u_z = torch.autograd.grad(model[0].generator(z).sum(), z)[0]\n",
    "                u_z = normalize(u_z, step_size, relative)\n",
    "            elif model_name == \"ChemSpace\":\n",
    "                u_z = normalize(model[0], step_size, relative)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model_name {model_name}\")\n",
    "\n",
    "        if properties[sem_idx] in [\"sa\", \"molwt\"]:\n",
    "            u_z = -u_z\n",
    "        if step < 0:\n",
    "            z = z - u_z\n",
    "        else:\n",
    "            z = z + u_z\n",
    "\n",
    "# original_image, original_label = synthesize_original(z, vae, dm, device)\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.imshow(original_image)\n",
    "# plt.title('Initial Molecule\\n'+original_label)\n",
    "# plt.axis('off')\n",
    "# print('')\n",
    "\n",
    "final_image, final_label = synthesize_final(z, vae, dm, device)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(final_image)\n",
    "plt.title('After Traversal:\\n'+final_label)\n",
    "plt.axis('off')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06943b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "VINA_EXECUTABLE = './vina_1.2.7_linux_x86_64'\n",
    "CONFIG_FILE = 'pdb_files/conf.txt'\n",
    "INPUT_CSV = 'master_df.csv'\n",
    "OUTPUT_CSV = 'results.csv'\n",
    "LIGAND_PREP_DIR = 'ligand_files'\n",
    "\n",
    "os.makedirs(LIGAND_PREP_DIR, exist_ok=True)\n",
    "\n",
    "def prepare_ligand_and_get_hac(smiles_string, ligand_id):\n",
    "    \"\"\"\n",
    "    Converts a SMILES string to a 3D PDBQT file, handling potential errors.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles_string)\n",
    "    if mol is None:\n",
    "        print(f\"Warning: RDKit could not parse SMILES for {ligand_id}: {smiles_string}\")\n",
    "        return None, None\n",
    "\n",
    "    hac = mol.GetNumHeavyAtoms()\n",
    "    mol = Chem.AddHs(mol)\n",
    "    \n",
    "    status = AllChem.EmbedMolecule(mol, AllChem.ETKDGv3())\n",
    "    if status == -1:\n",
    "        print(f\"Warning: RDKit could not generate a 3D conformer for {ligand_id}. Skipping.\")\n",
    "        return None, None\n",
    "        \n",
    "    AllChem.UFFOptimizeMolecule(mol)\n",
    "    \n",
    "    sdf_path = os.path.join(LIGAND_PREP_DIR, f\"{ligand_id}.sdf\")\n",
    "    writer = Chem.SDWriter(sdf_path)\n",
    "    writer.write(mol)\n",
    "    writer.close()\n",
    "    \n",
    "    pdbqt_path = os.path.join(LIGAND_PREP_DIR, f\"{ligand_id}.pdbqt\")\n",
    "    subprocess.run(\n",
    "        ['obabel', sdf_path, '-O', pdbqt_path, '-p', '--partialcharge', 'gasteiger'],\n",
    "        check=True,\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    return pdbqt_path, hac\n",
    "\n",
    "def run_vina_docking(ligand_pdbqt_path, ligand_id):\n",
    "    \"\"\"\n",
    "    Runs AutoDock Vina and parses the output to get the binding affinity.\n",
    "    This version uses robust parsing to correctly find the score.\n",
    "    \"\"\"\n",
    "    output_pose_path = os.path.join(LIGAND_PREP_DIR, f\"{ligand_id}_out.pdbqt\")\n",
    "    \n",
    "    command = [\n",
    "        VINA_EXECUTABLE,\n",
    "        '--config', CONFIG_FILE,\n",
    "        '--ligand', ligand_pdbqt_path,\n",
    "        '--out', output_pose_path,\n",
    "    ]\n",
    "\n",
    "    result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "    \n",
    "    output_lines = result.stdout.strip().split('\\n')\n",
    "    \n",
    "    # --- Final, Corrected Parsing Logic ---\n",
    "    for line in output_lines:\n",
    "        # The best score is always on the line that starts with '1' in the results table\n",
    "        if line.strip().startswith('1'):\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    affinity = float(parts[1])\n",
    "                    return affinity # Success! Return the score.\n",
    "                except (ValueError, IndexError):\n",
    "                    continue # Ignore lines that might start with 1 but aren't score lines\n",
    "    \n",
    "    # If the loop finishes and no score was found, Vina truly produced no results.\n",
    "    print(f\"\\nWarning: Could not parse a valid score for {ligand_id} from Vina's output.\\n\")\n",
    "    return None\n",
    "from tqdm import tqdm\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"Starting docking for {len(df)} molecules...\")\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        smiles = row['smiles']\n",
    "        ligand_id = f\"ligand_{index}\"\n",
    "                \n",
    "        try:\n",
    "            ligand_path, heavy_atoms = prepare_ligand_and_get_hac(smiles, ligand_id)\n",
    "            \n",
    "            if ligand_path is None:\n",
    "                results.append({**row.to_dict(), 'binding_affinity': None, 'heavy_atoms': None, 'ligand_efficiency': None, 'error': 'Ligand preparation failed'})\n",
    "                continue\n",
    "\n",
    "            binding_affinity = run_vina_docking(ligand_path, ligand_id)\n",
    "\n",
    "            ligand_efficiency = None\n",
    "            if binding_affinity is not None and heavy_atoms > 0:\n",
    "                ligand_efficiency = binding_affinity / heavy_atoms\n",
    "            \n",
    "            results.append({\n",
    "                **row.to_dict(),\n",
    "                'binding_affinity': binding_affinity,\n",
    "                'heavy_atoms': heavy_atoms,\n",
    "                'ligand_efficiency': ligand_efficiency\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred for {ligand_id}: {e}\")\n",
    "            results.append({**row.to_dict(), 'binding_affinity': None, 'heavy_atoms': None, 'ligand_efficiency': None, 'error': str(e)})\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "    print(f\"\\nDocking complete! Results saved to {OUTPUT_CSV}\")\n",
    "    print(\"Temporary ligand files are in the 'ligand_files' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e782ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "OUTPUT_CSV = 'results.csv'\n",
    "results_df = pd.read_csv(OUTPUT_CSV)\n",
    "results_df['molecule_num'] = results_df.groupby('optimization_method').cumcount()\n",
    "pivot_df = results_df.pivot(index='optimization_method', columns='molecule_num', values='binding_affinity')\n",
    "pivot_df.columns = [f'Score_{i+1}' for i in pivot_df.columns]\n",
    "\n",
    "cutoff = np.partition(pivot_df.values.flatten(), 5)[5]\n",
    "\n",
    "def highlight_top5_smallest(val):\n",
    "    return 'font-weight: bold' if val <= cutoff else ''\n",
    "\n",
    "styled_df = pivot_df.style.applymap(highlight_top5_smallest)\\\n",
    "                            .format(\"{:.2f}\")\\\n",
    "                            .set_caption(\"Binding Affinity Scores (kcal/mol) by Method (Top 5 Smallest Bolded)\")\n",
    "\n",
    "styled_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
